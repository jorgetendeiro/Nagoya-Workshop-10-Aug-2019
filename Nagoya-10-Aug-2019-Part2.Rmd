---
title: "From Frequentist Problems Towards Bayesian Solutions"
subtitle: "Part 2: Introduction to Bayesian statistics (with JASP)"
author: "Jorge N. Tendeiro"
date: "10 August 2019<br><br>Slides at: [https://rebrand.ly/Nagoya2019-Part2](https://rebrand.ly/Nagoya2019-Part2)<br>GitHub: [https://github.com/jorgetendeiro/Nagoya-Workshop-10-Aug-2019 ](https://github.com/jorgetendeiro/Nagoya-Workshop-10-Aug-2019)"
# https://jorgetendeiro.github.io/Nagoya-Workshop-10-Aug-2019/Nagoya-10-Aug-2019-Part2.html
output: 
  ioslides_presentation:
    smaller: true
    widescreen: true
    transition: .4
    css: [style/slides.css, style/toc-style.css]
    includes:
      in_header: style/TOC_generator.js.wrapper
bibliography: references.bib
csl: style/apa-old-doi-prefix.csl
---

```{r setup, include = FALSE, fig.align = 'center'}
knitr::opts_chunk$set(
  echo      = FALSE, 
  fig.cap   = '', 
  out.extra = 'style="background-color: #000000; padding:5px; display:block; margin:auto;"'
  ) 
```

<!-- To print to PDF, run: 
pagedown::chrome_print("Nagoya-10-Aug-2019-Part2.html")
-->

## Today
<!-- add div element table of content for javascript to fill -->
<div id="toc"></div>

# Suggested reading material {.section}

## Introduction to Bayes {.build}
<span style="color:green">Papers:</span>

- @etz2018: The "Harry Potter" paper.<br>
*Very* accessible introduction, with examples.
- @etz2018a: "How to become a Bayesian in eight easy steps: An annotated reading list".<br>
Yes, Alexander Etz writes 'readible' papers *very* well. Strongly advised read, but it takes quite some time to process.
- @kruschke2013: Besides providing an excellent introduction to core concepts, Kruschke offers a discussion over the "testing" versus "estimation" tension. I personally like Kruschke's position on this matter.

<span style="color:green">Books:</span>

- @kruschke2015: The "puppies" book.<br>
Accessible book, with plenty of examples and code. Perfect as a first pick.
- @mcelreath2016: From what I read thus far, this book is a jewel.
- @lambert2018: I'm currently half way. Seems perfect for teaching (hence learning!).
- @gelman2014: More advanced read (perhaps not the first pick), but truly a master piece.

## Learn about JASP {.build}

<span style="color:green">Website</span><br> [https://jasp-stats.org/how-to-use-jasp/](https://jasp-stats.org/how-to-use-jasp/).<br>
It includes:

- [Tutorial section](https://jasp-stats.org/how-to-use-jasp/)
- [YouTube channel](http://www.youtube.com/channel/UCSulowI4mXFyBkw3bmp7pXg) to see (and hear) it in action.
- [Forum](http://forum.cogsci.nl/index.php?p=/categories/jasp-bayesfactor) to post lingering questions.
- [Teaching with JASP](https://jasp-stats.org/teaching-with-jasp/) includes much more material.

<br>
<span style="color:green">Video tutorials</span><br>
Etz (who else?) is making videos as we speak.<br>
[https://alexanderetz.com/jasp-tutorials/](https://alexanderetz.com/jasp-tutorials/).

<br>
<span style="color:green">Papers</span>

- @marsman2017[, in *European Journal of Developmental Psychology*].
- @wagenmakers2018[, in a special issue in *Psychonomic Bulletin & Review*].

# Frequentist *versus* Bayes {.section}
## Frequentist paradigm {.build}
Concept of <span style="color:green">probability</span>:

- Long-run frequency of a <span style="color:red">procedure</span>.

> The probability of a fair coin landing up heads is 50%.

- One cannot state anything about one single event in the long-run sequence.

> What is the probability that the <span style="color:red">next</span> coin toss lands heads?

- Recall the definitions of a <span style="color:red">$p$-value</span> and <span style="color:red">confidence interval</span>:
They are based on long-run frequencies.<br>
<span style="color:green">Conclusion:</span> What can we really conclude from one $p$-value or one confidence interval?...


## Bayesian paradigm {.build}
Concept of <span style="color:green">probability</span>:

- Degree of belief.
- Expression of uncertainty about the true state of affairs.
- Subjective: Different people have different beliefs.
- Data are used to update one's belief, by means of the *laws of probability*.
- Applies to both single and repetitive events.

<br>
But how do we update our belief in light of data?

# Bayes' rule {.section}

## Bayes' rule {.build}
Let $\mathcal{A}$ denote something we want to study.
<br>
This can be:

- A parameter, like the mean $\mu$ of a population.
- A hypothesis, like $\mu > 100$.

Bayes' rule:

<p align="center"><span style="color:green">
$p(\mathcal{A}|\text{data}) = \frac{p(\mathcal{A})p(\text{data}|\mathcal{A})}{p(\text{data})}$
</span></p>

- $p(\mathcal{A})$: <span style="color:red">Prior</span> probability.
- $p(\text{data}|\mathcal{A})$: Data <span style="color:red">likelihood</span>.
- $p(\text{data})$: Marginal likelihood.
- $p(\mathcal{A}|\text{data})$: <span style="color:red">Posterior</span> probability.

## Bayes' rule and frequentism
<span style="color:red">Important:</span>

- The Bayes' rule is a mathematical necessity, it follows from the axioms of probability.
- Frequentists do not dispute this formula!

## Bayes' rule and model comparison {.build}
Say we have <span style="color:red">in total</span> two competing hypotheses, $\mathcal{H}_0$ and $\mathcal{H}_1$.

We can apply the Bayes' rule to either hypothesis:

<p align="center">
$p(\mathcal{H}_0|\text{data}) = \frac{p(\mathcal{H}_0)p(\text{data}|\mathcal{H}_0)}{p(\text{data})}
\quad,\quad
p(\mathcal{H}_1|\text{data}) = \frac{p(\mathcal{H}_1)p(\text{data}|\mathcal{H}_1)}{p(\text{data})}$.
</p>

Now divide both equations:

<p align="center"><span style="color:green">
$\underset{\text{Posterior odds}}{\underbrace{\frac{p(\mathcal{H}_0|\text{data})}{p(\mathcal{H}_1|\text{data})}}} =
\underset{\text{Prior odds}}{\underbrace{\frac{p(\mathcal{H}_0)}{p(\mathcal{H}_1)}}}
\times
\underset{BF_{01}}{\underbrace{\frac{p(\text{data}|\mathcal{H}_0)}{p(\text{data}|\mathcal{H}_1)}}}$
</span></p>

where $BF_{01}$ is the <span style="color:red">Bayes factor</span>.

The Bayes factor is a measure of the *relative evidence* in the data for either model.
E.g., if $BF_{01} = 5$:

- The data are 5 times as likely under $\mathcal{H}_0$ than under $\mathcal{H}_1$.
- After looking at the data, we now support $\mathcal{H}_0$ five times as much.

## Bayes factor
A whole lot can be said about Bayes factors.

They have fervent followers [e.g., @kass1995; @dienes2014; @morey2016b; @wagenmakers2018a].

But there are also critics, including myself [@tendeiro2019a].

# JASP {.section}

## Note of caution

JASP is 'Bayes factor'-oriented.

I personally dislike it, as I think parameter estimation offers a far a clearer, all-inclusive, paradigm.

- To see why I think this, see our preprint: @kiers2019a.

## Worked-out example
Let's jump to JASP now!

The pet example that I will use is the first experiment of @bem2011:

<p align="center"><span style="color:green">Precognitive detection of erotic stimuli</span>.</p>

- $n = 100$ (50 men, 50 women), 36 trials per subject.
- In each trial:
    - Two curtains shown side by side.
    - One curtain hides a picture, the other hides a blank wall.
    - Erotic and nonerotic pictures randomly intermixed.

Main research hypothesis:

> Subjects are able to "feel" where the erotic pictures are more often than chance (!!!).

## Some results from Bem (2011)

> Across all 100 sessions, participants correctly identified the future position of the erotic pictures significantly more frequently than the 50% hit rate expected by chance: 53.1%, $t(99) = 2.51$, $p = .01$, $d = 0.25$. In contrast, their hit rate on the nonerotic pictures did not differ significantly from chance: 49.8%, $t(99) = -0.15$, $p = .56$.

> The difference between erotic and nonerotic trials was itself significant, $t_\text{diff}(99) = 1.85$, $p = .031$, $d = 0.19$.

> (...) the correlation between stimulus seeking and psi performance was .18 ($p = .035$).

# Summary of today's workshop

## Main ideas: Replication crisis {.build}

Plenty of problems in psychological research have been identified throughout the years:

- Results do not replicate.
- Bias.
- QRPs.
- CIs and $p$-values poorly understood and often misused.

<br>
Psychology is currently in the middle of a revolution. Several solutions are being worked out:

- Preregistration.
- Registered reports.
- Open data, materials.
- Embrace uncertainty. Avoid dichotomous thinking.
- Better statistical analyses. In particular: <span style="color:red">Stop using NHST.</span>

<br>
The entire research ecosystem is picking up on these changes <span style="color:red">fast</span>!
    
## Main ideas: Bayesian statistics {.build}
Bayesian statistics is gaining traction as a viable alternative.

- Quantification and accumulation of evidence.
- Logical updating of belief.
- Avoid long-standing fallacies of classical statistics.

<br>
JASP in particular is one very friendly software that can ease the use of Bayesian statistics.

<br>
JASP is mostly model comparison/ hypothesis testing based.

<br>
I suggested that there is valid criticism against *only* hypothesis testing<br>
[e.g., @tendeiro2019a; @kiers2019a].

<br>
Beyond JASP, I advocate model fitting, parameter estimation, and reporting summaries of posterior distributions.

<br>
Tools needed: MCMC sampling (e.g., JAGS, Stan).

<style type="text/css">
slides > slide { overflow: scroll; }
}
</style>

# References {.section}

----
